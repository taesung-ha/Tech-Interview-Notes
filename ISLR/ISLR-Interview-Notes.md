# ğŸ“˜ ISLR Interview Notes â€“ High-Yield Concepts

A distilled summary of key ISLR concepts that are frequently asked in data science and quant/stat interviews.

---

## ğŸ“Œ 1. Linear Regression

### ğŸ”¹ What are the assumptions of linear regression?
- Linearity
- Independence
- Homoscedasticity
- Normality of residuals
- No multicollinearity (in multiple regression)

### ğŸ”¹ How is the coefficient Î²â‚ interpreted?
- Î²â‚ represents the expected change in the response variable for a one-unit increase in the predictor, holding other variables constant.

### ğŸ”¹ What is the difference between RÂ² and adjusted RÂ²?
- RÂ² measures explained variance.
- Adjusted RÂ² penalizes for adding non-informative predictors.

**ğŸ’¡ Tip:**  
In interviews, always mention that assumptions should be *checked*, not just assumed.

---

## ğŸ“Œ 2. Classification â€“ Logistic Regression

### ğŸ”¹ Why canâ€™t we use linear regression for classification?
- Predicted values are not bounded between 0 and 1.
- Linear regression doesnâ€™t model probability well for binary outcomes.

### ğŸ”¹ What is the log-odds (logit)?
- `log(p / (1 - p))` where `p` is the predicted probability.
- Logistic regression models the log-odds as a linear function of predictors.

**ğŸ’¡ Tip:**  
Mention maximum likelihood estimation (MLE) when asked about model fitting.

---

## ğŸ“Œ 3. Resampling Methods

### ğŸ”¹ What is the purpose of cross-validation?
- To estimate model performance and prevent overfitting.
- Helps select tuning parameters (e.g., regularization strength in Ridge/Lasso).

### ğŸ”¹ Difference between LOOCV and k-fold CV?
- LOOCV: low bias, high variance.
- k-fold (e.g., k=5, 10): trade-off between bias and variance.

---

## ğŸ“Œ 4. Regularization â€“ Ridge vs Lasso

| Method  | Penalty         | Shrinks Coefficients | Can Zero Out Features? |
|---------|------------------|----------------------|------------------------|
| Ridge   | L2 (sum of squares) | Yes                  | No                     |
| Lasso   | L1 (sum of abs)     | Yes                  | Yes (feature selection) |

**ğŸ’¡ Tip:**  
Lasso is often favored in high-dimensional settings (e.g., p >> n).

---

## ğŸ“Œ 5. Bias-Variance Tradeoff

### ğŸ”¹ Explain bias-variance tradeoff.
- High bias: underfitting
- High variance: overfitting
- Ideal model minimizes total test error (biasÂ² + variance + irreducible error)

**Interview phrasing tip:**  
Say â€œthe tradeoff governs the expected test errorâ€ and mention model complexity control.

---

## ğŸ“Œ 6. Decision Trees & Bagging

### ğŸ”¹ Why are trees prone to overfitting?
- Trees are very flexible and fit noise unless pruned.

### ğŸ”¹ What does bagging do?
- Reduces variance by averaging over bootstrapped models.

---

## ğŸ“Œ 7. Random Forests

### ğŸ”¹ How is Random Forest different from bagging?
- Adds randomness: at each split, only a subset of predictors is considered.

### ğŸ”¹ Variable importance?
- Mean decrease in impurity
- Mean decrease in accuracy (OOB score drop)

---

## ğŸ“Œ 8. Support Vector Machines (Optional)

### ğŸ”¹ What is the margin in SVM?
- Distance between the separating hyperplane and the closest points (support vectors).

### ğŸ”¹ What is the kernel trick?
- Projects data into higher dimensions without explicitly computing it.

---

## ğŸ“Œ 9. Model Evaluation Metrics

| Metric      | Type        | Use Case                                 |
|-------------|-------------|------------------------------------------|
| Accuracy    | Classification | When classes are balanced              |
| Precision   | Classification | When false positives are costly       |
| Recall      | Classification | When false negatives are costly       |
| F1 Score    | Classification | When precision/recall are both needed |
| RMSE / MAE  | Regression   | RMSE penalizes outliers more            |
| RÂ²          | Regression   | Explains variance                       |

---

## ğŸ§  Final Tips for Interview

- Be able to explain *why* you use a method, not just how.
- Use visuals or metaphors if you're comfortable (e.g., â€œlasso squeezes small features to zeroâ€).
- Always discuss assumptions, limitations, and when you would **not** use a method.
